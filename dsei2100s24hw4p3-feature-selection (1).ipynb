{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aa4a39cd-ed35-4ad8-aa61-28d8a420a3c1",
      "metadata": {},
      "source": [
        "# HW04: Problem 3: Feature Selection\n",
        "\n",
        "## Description\n",
        "\n",
        "In this problem we will work with the diabetes dataset from sklearn. This data set is for a regression problem where 10 features are used to predict the progression of diabetes. The dataset is described in more detail [here](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset). You task here is to use multiple techniques of feature selection to try to interpret the strength of the features in the dataset. You will need to use the following techniques:\n",
        "\n",
        "- Pearson correlation coefficient using r_regression from sklearn (univariate feature selection)\n",
        "- Mutual information using mutual_info_regression from sklearn (univariate feature selection)\n",
        "- Random forest feature importance using RandomForestRegressor from sklearn (multivariate feature selection)\n",
        "- Recursive feature elimination using sklearn.feature.selection.RFE with a Support Vector Regressor SVR (multivariate feature selection)\n",
        "\n",
        "For each method you will need to plot the feature importance as a bar graph. The importance goes by different names in different algorithms. For example, in r_regression it is just the output (r value) and the mutual information in mutual_info_regression. In random variable it is called feature_importance_ and in RFE it is the ranking_. The bar graph will be sorted from most important features to least important features, with the y value being the importance of that feature, and the x value being the rank but labeled with the feature name.\n",
        "You will also need to print out the top 5 features for each method. You will need to use the following code to load the data and split it into training and testing sets. You will need to use the training set for all of the feature selection methods.\n",
        "\n",
        "* Are there 3 features that are selected in the top 5 by all 4 methods? \n",
        "* If so, what are they? \n",
        "* If not, what are the 3 features that are selected by the most methods? \n",
        "* How would it be possible that univariate methods might select different features than multivariate methods? \n",
        "* How does dependence between features affect the feature selection methods?\n",
        "\n",
        "For good habits, make sure you split your code into training and testing. You may not even use the testing data but when you do any analysis such as feature selection, remember you must not use the testing data. You should also make sure you use the same random seed for all of your feature selection methods so that you can compare the results.\n",
        "\n",
        "## Hints: In sorting features you use \"arg\" sort. This will return the indices of the sorted array. You can use these indices to sort the feature names.\n",
        "\n",
        "This kind of code will be useful for plotting the bar graph:\n",
        "\n",
        "```python\n",
        "r_inds = np.argsort(np.abs(r_importance))[::-1]\n",
        "fig, ax = plt.subplots()\n",
        "rank = np.arange(len(data.feature_names))\n",
        "ax.bar(rank, r_importance[r_inds])\n",
        "ax.set_xticks(rank)\n",
        "ax.set_xticklabels(np.array(data.feature_names)[r_inds])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "24211c5d-8831-4401-a941-1ae3c6e33e09",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Some imports you will need\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "from numpy import corrcoef\n",
        "import seaborn as sns\n",
        "from sklearn.feature_selection import r_regression\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68bffa0c-62aa-4da5-a81a-db0992f58fe6",
      "metadata": {},
      "source": [
        "## Loading and preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "1cf42116-e3c2-4343-87e8-1e6c7986f51c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the diabetes data set as X, y\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "# Load the diabetes data set as data to read the description\n",
        "data = load_diabetes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "1709df79",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(442, 10)\n",
            "(442,)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "6b91fa04-a015-402c-b439-dbbcd2638a1a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 442\\n\\n:Number of Attributes: First 10 columns are numeric predictive values\\n\\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n:Attribute Information:\\n    - age     age in years\\n    - sex\\n    - bmi     body mass index\\n    - bp      average blood pressure\\n    - s1      tc, total serum cholesterol\\n    - s2      ldl, low-density lipoproteins\\n    - s3      hdl, high-density lipoproteins\\n    - s4      tch, total cholesterol / HDL\\n    - s5      ltg, possibly log of serum triglycerides level\\n    - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n'"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print out the DESCR attribute to inpect the variables\n",
        "data.DESCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "1b6f5d6f-4a81-4b97-9ca3-a15ac360e52e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print the array of feature names\n",
        "data.feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "5130ef8b-7364-42c5-bfbe-abcabe5e5592",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the Data into train/testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "8eb090c2-a531-4432-b10a-3192a0d25383",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(331, 10)\n",
            "(331,)\n",
            "(111, 10)\n",
            "(111,)\n"
          ]
        }
      ],
      "source": [
        "# Check the shapes\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "964289ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert into DF to plot it\n",
        "df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
        "df['target'] = data.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1e3968fc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>bp</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>s6</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.038076</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.061696</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>-0.044223</td>\n",
              "      <td>-0.034821</td>\n",
              "      <td>-0.043401</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.019907</td>\n",
              "      <td>-0.017646</td>\n",
              "      <td>151.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001882</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.051474</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>-0.008449</td>\n",
              "      <td>-0.019163</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.068332</td>\n",
              "      <td>-0.092204</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.085299</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.044451</td>\n",
              "      <td>-0.005670</td>\n",
              "      <td>-0.045599</td>\n",
              "      <td>-0.034194</td>\n",
              "      <td>-0.032356</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.002861</td>\n",
              "      <td>-0.025930</td>\n",
              "      <td>141.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.089063</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.011595</td>\n",
              "      <td>-0.036656</td>\n",
              "      <td>0.012191</td>\n",
              "      <td>0.024991</td>\n",
              "      <td>-0.036038</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.022688</td>\n",
              "      <td>-0.009362</td>\n",
              "      <td>206.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.005383</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.036385</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>0.003935</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>0.008142</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.031988</td>\n",
              "      <td>-0.046641</td>\n",
              "      <td>135.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        age       sex       bmi        bp        s1        s2        s3  \\\n",
              "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
              "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
              "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
              "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
              "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
              "\n",
              "         s4        s5        s6  target  \n",
              "0 -0.002592  0.019907 -0.017646   151.0  \n",
              "1 -0.039493 -0.068332 -0.092204    75.0  \n",
              "2 -0.002592  0.002861 -0.025930   141.0  \n",
              "3  0.034309  0.022688 -0.009362   206.0  \n",
              "4 -0.002592 -0.031988 -0.046641   135.0  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f99a2c03-e007-491f-96cd-a13ff768a319",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Do a pair plot \n",
        "sns.pairplot(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9e62670-d3e4-4359-a24a-65161d4c61f1",
      "metadata": {},
      "source": [
        "Q: What does the EDA tell you about the data?\n",
        "\n",
        "A Here: The first thing that pops out to me is that features 's2' (low density lipoprotein) and 's1' (total serum cholesterol) have a strong positive correlation. Additionally, the feature 's4' (total cholesterol) seems to have a strange clustering pattern and has a negative correlation wth 's3' (high density lipoprotein). The rest of the features have negligible correlations with each other."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f40b553-b6f6-4fd7-866a-d00ac8875a7c",
      "metadata": {},
      "source": [
        "## Univariate feature selection with r_regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c54fe6d-df5f-4117-bf4a-5ec10f30274f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use r_regression to get the feature importance, sort by the absolute value\n",
        "reg = r_regression(X, y)\n",
        "r_inds = np.argsort(np.abs(reg))[::-1]\n",
        "\n",
        "# but show the signed value on y and label on x by variable name\n",
        "# Should be a bar graph\n",
        "fig, ax = plt.subplots()\n",
        "rank = np.arange(len(data.feature_names))\n",
        "ax.bar(rank, reg[r_inds])\n",
        "ax.set_xticks(rank)\n",
        "ax.set_xticklabels(np.array(data.feature_names)[r_inds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "6c2289c4-a12c-4fec-b810-c31b12927459",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bmi: 0.5864501344746885\n",
            "s5: 0.5658825924427444\n",
            "bp: 0.44148175856257105\n",
            "s4: 0.4304528847447733\n",
            "s3: -0.39478925067091875\n"
          ]
        }
      ],
      "source": [
        "# print the top 5 features according to r_regression?\n",
        "for i in range(5):\n",
        "    feature_index = r_inds[i]\n",
        "    feature_name = data.feature_names[feature_index]\n",
        "    feature_importance = reg[feature_index]\n",
        "    print(f\"{feature_name}: {feature_importance}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f727134c-16a5-406e-a19e-d0eebcf85348",
      "metadata": {},
      "source": [
        "## Univariate feature selection with mutual information using mutual_info_regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce8e90e9-28be-4523-9517-17f9b7837c82",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use mutual_info_regression to get the feature importance, sort by the absolute value\n",
        "mut = mutual_info_regression(X,y)\n",
        "m_inds = np.argsort(abs(mut))[::-1]\n",
        "\n",
        "# but show the signed value on y and label on x by variable name\n",
        "# Should be a bar graph\n",
        "fig, ax = plt.subplots()\n",
        "rank = np.arange(len(data.feature_names))\n",
        "ax.bar(rank, mut[m_inds])\n",
        "ax.set_xticks(rank)\n",
        "ax.set_xticklabels(np.array(data.feature_names)[m_inds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "fa0b71bf-f413-48c8-833e-21ec48277f27",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bmi: 0.1738929602380801\n",
            "s5: 0.14720030221034452\n",
            "s6: 0.09382337018867837\n",
            "s4: 0.08886620231126585\n",
            "s3: 0.06812562637580832\n"
          ]
        }
      ],
      "source": [
        "# What are the top 5 features according to mutual_info_regression?\n",
        "# print the top 5 features according to r_regression?\n",
        "for i in range(5):\n",
        "    feature_index = m_inds[i]\n",
        "    feature_name = data.feature_names[feature_index]\n",
        "    feature_importance = mut[feature_index]\n",
        "    print(f\"{feature_name}: {feature_importance}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72fecc71-72fd-4f33-ac39-ad97d593983e",
      "metadata": {},
      "source": [
        "## Multivariate feature selection with Random Forest feature_importance_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "592443ba-5f5f-4f48-9488-25eaeb699ce7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use random forest feature_importance_ to get the feature importance, sort by the absolute value\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "f_imp = model.feature_importances_\n",
        "f_imp_sort = np.argsort(abs(f_imp))[::-1]\n",
        "\n",
        "# but show the signed value on y and label on x by variable name\n",
        "# Should be a bar graph\n",
        "fig, ax = plt.subplots()\n",
        "rank = np.arange(len(data.feature_names))\n",
        "ax.bar(rank, f_imp[f_imp_sort])\n",
        "ax.set_xticks(rank)\n",
        "ax.set_xticklabels(np.array(data.feature_names)[f_imp_sort])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "fdb44bb7-c6e1-499e-863b-da7417152b3a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bmi: 0.38156187079306575\n",
            "s5: 0.21876907497850503\n",
            "bp: 0.10642955527230183\n",
            "s6: 0.06376345502233687\n",
            "age: 0.05665432119508048\n"
          ]
        }
      ],
      "source": [
        "# What are the top 5 features according to random forest feature_importance_?\n",
        "for i in range(5):\n",
        "    feature_index = f_imp_sort[i]\n",
        "    feature_name = data.feature_names[feature_index]\n",
        "    feature_importance = f_imp[feature_index]\n",
        "    print(f\"{feature_name}: {feature_importance}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf3e8967-a1ad-42e2-99b4-5119c4f2c0b6",
      "metadata": {},
      "source": [
        "## Multivariate feature selection with recursive feature elimination (RFE) using a support vector regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30102205",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use recursive feature elimination (RFE) with a support vector regressor\n",
        "svr = SVR(kernel='linear')\n",
        "rfe = RFE(estimator = svr, n_features_to_select=5)\n",
        "rfe.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1c92c89-8842-4b38-a3fd-b2413b7459f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# to get the feature importance, sort by the absolute value\n",
        "rfe_rank = rfe.ranking_\n",
        "rfe_rank_sort = np.argsort(abs(rfe_rank))[::-1]\n",
        "\n",
        "# but show the signed value on y and label on x by variable name\n",
        "# Should be a bar graph\n",
        "fig, ax = plt.subplots()\n",
        "rank = np.arange(len(data.feature_names))\n",
        "ax.bar(rank, rfe_rank[rfe_rank_sort])\n",
        "ax.set_xticks(rank)\n",
        "ax.set_xticklabels(np.array(data.feature_names)[rfe_rank_sort])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "f77104ef-bbf1-4788-8b3c-6077937cceb4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sex: 6\n",
            "s2: 5\n",
            "age: 4\n",
            "s1: 3\n",
            "s6: 2\n"
          ]
        }
      ],
      "source": [
        "# What are the top 5 features according to RFE with SVR?\n",
        "for i in range(5):\n",
        "    feature_index = rfe_rank_sort[i]\n",
        "    feature_name = data.feature_names[feature_index]\n",
        "    feature_importance = rfe_rank[feature_index]\n",
        "    print(f\"{feature_name}: {feature_importance}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2be0d412-fe5f-4d60-b45f-061669037e64",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "Q1: Are there 3 features that are selected in the top 5 by all 4 methods?\n",
        "\n",
        "A1: No, not all four methods included the same feature in their top 5 features.\n",
        "\n",
        "Q2: If so, what are they? / If not, what are the 3 features that are selected by the most methods? \n",
        "\n",
        "A2: Feature 's6' was chosen by all methods besides the method that used r_regression. The feature 'bmi' was chosen by all methods except for RFE.\n",
        "\n",
        "Q3: How would it be possible that univariate methods might select different features than multivariate methods?\n",
        "\n",
        "A3: Univariate methods do not take into account how some features may be related to each other, unlike multivariate methods, and therefore cannot detect redundant features as easily.\n",
        "\n",
        "Q4: How does dependence between features affect the feature selection methods?\n",
        "\n",
        "Q4: If features have some sort of dependence between each other, univariate feature selection methods won't be able to take that into account and lose possibly important information. In this case, it is probably best to use a multivariate feature selection method which can take these important relationships into account."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
